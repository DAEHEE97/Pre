{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23698deb",
   "metadata": {},
   "source": [
    "# 6기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7c482",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2831f2",
   "metadata": {},
   "source": [
    "## 6-1) numpy 내적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4beb4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eda2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 열 벡터 (3,1)\n",
    "x = np.array([[1],[2],[3]]) \n",
    "y = np.array([[4],[5],[6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8011cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [2],\n",
       "        [3]]),\n",
       " array([[4],\n",
       "        [5],\n",
       "        [6]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360eb197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T # (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a2e3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9ec481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32]]\n",
      "[[ 4  8 12]\n",
      " [ 5 10 15]\n",
      " [ 6 12 18]]\n"
     ]
    }
   ],
   "source": [
    "# 스칼라\n",
    "print( x.T @ y ) # (1,3) (3,1) \n",
    "\n",
    "\n",
    "# 3x3 해열\n",
    "print( y @ x.T ) # (3,1) (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e387804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y @ x.T).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a0c9d",
   "metadata": {},
   "source": [
    "- 기존 x.T, y 2D 배열 형태 일관성을 유지하는 것이 NumPy의 일반적인 동작입니다. \n",
    "- 내적의 결과또한 NumPy에서 2D 배열 형태로 반환됩니다. \n",
    "- 따라서 x.T @ y의 결과가 array([[32]])로 표시됩니다. \n",
    "- 이것은 2D 배열이지만 내부에는 하나의 스칼라 값 32가 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9471e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2ce615",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3]) \n",
    "y = np.array([4,5,6]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8495f931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape # 원소의 개수를 3개가진 1차원 배열, 행벡터, 열벡터라고 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54e897c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (3,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148513be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (3,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T.shape,y.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb0e62",
   "metadata": {},
   "source": [
    "- NumPy에서 1차원 배열은 일반적으로 행 벡터나 열 벡터와는 다르게 처리됩니다. \n",
    "\n",
    "- 1차원 배열의 `shape` 속성은 `(n,)` 형태로 나타납니다. \n",
    "\n",
    "- 여기서 `n`은 배열의 원소 수를 의미합니다. 따라서 `x = np.array([1, 2, 3])`의 경우 `x.shape`를 실행하면 `(3,)`로 표시되며, 이는 1차원 배열이며 원소가 3개라는 것을 나타냅니다.\n",
    "\n",
    "- 이러한 표기는 NumPy에서의 일반적인 관례입니다. 1차원 배열은 열 벡터나 행 벡터로 간주하지 않고, 간단히 1차원 배열로 처리됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba209a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.T @ y, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcec5d4",
   "metadata": {},
   "source": [
    "- x와 y는 원래 열 벡터(column vector)입니다. \n",
    "\n",
    "- 1차원 배열로 표현되어 있더라도 열 벡터의 개념을 가지고 있으며, 전치(transpose) 연산을 수행하더라도 열 벡터의 형태는 변하지 않습니다. \n",
    "\n",
    "- 따라서 내적을 계산할 때 x.T와 y를 내적하든, y와 x.T를 내적하든 결과는 동일하며, 그 결과는 스칼라 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35eb14cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y @ x.T # (3,1) (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb94d72c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b786388",
   "metadata": {},
   "source": [
    "## 6-2) 선형 독립\n",
    "    - 서로 직교하는 영벡터가 아닌 N개의 벡터가 선형 독립 임을 증명하는 과정\n",
    "\n",
    "만약,벡터 v1,v2,...vn 에 대해 선형 종속을 만드는 모두 0 이아닌 계수(c1,c2,.., cN)이 존재 한다면 ( A )이 성립 한다.\n",
    "\n",
    "또한 ( A ) 에 vi (i = 1, ..., N) 을 내적한 값이 0 이 성립한다.\n",
    "\n",
    "이 때, 이 벡터들은 서로 직교하므로, i != j 인 모든 j에 대해 ( B ) 이다. \n",
    "\n",
    "따라서, vi(c1v1 + c2v2 + ... + cnvn) = civi^Tvi = ci||vi||^2 = 0\n",
    "\n",
    "이때,이 벡터들은 영벡터가 아니므로 ci = 0이고, 이는 모든 i = 1,... N에 대해 성립하므로, 선형 종속을 만드는 모두 0이 아닌 계수 ci,c2,..,,cN 는 존재하지 않는다.\n",
    "\n",
    "따라서 벡터 v1,v2,...,vn 는 독립 이다.\n",
    "\n",
    "\n",
    "- c1v1 + c2v2 + ... + cnvn = 0\n",
    "- vi^Tvj = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddb0e4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4f100",
   "metadata": {},
   "source": [
    "## 6-3) 특성방정식, 고윳값\n",
    "\n",
    "- det(A-xi) = 0 을 만족하는 x값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17ce08",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b45dd",
   "metadata": {},
   "source": [
    "## 6-4) 확률 변수\n",
    "\n",
    "- 이산형 확률 변수는 확률 변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 \"더해서\" 모델링합니다.\n",
    "    - 이산형 확률 변수의 확률을 모델링할 때는 각 이벤트의 확률을 합하여 전체 확률분포를 정의합니다. \n",
    "\n",
    "\n",
    "- 연속형 확률 변수는 \"적분해서\" 확률을 모델링합니다. \n",
    "    - 연속형 확률 변수의 확률은 확률 밀도 함수를 이용하여 구간을 적분함으로써 계산됩니다.\n",
    "\n",
    "\n",
    "\n",
    "- 밀도는 누적확률분포의 변화율을 모델링하며, 밀도 를 특정 구간에 대하여 정적분하면 이것이 연속형 확률변수가 특정 구간에 포함될 확률이 된다.\n",
    "\n",
    "- 몬테카를로 샘플링 방법은 변수 유형(이산형, 연속형)에 상관없이 모두 사용할 수 있다.\n",
    "    - 몬테카를로 샘플링은 확률 분포를 알 수 없을 때, 데이터를 이용하여 기대값을 계산하기 위해 사용하는 방법입니다. \n",
    "    - 몬테카를로는 이산형이든, 연속형이든 변수 유형에 상관없이 성립합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec18cbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed0d83",
   "metadata": {},
   "source": [
    "## 6-5) 정규화 방식\n",
    "\n",
    "\n",
    "- Overfitting : 오버비팅은 주로 매개변수가 많고 표현력이 높은 모델이나, 훈련 데이터가 적은 경우에 발생한다.\n",
    "    - Underfitting(과소적합)은 모델이 너무 단순하거나 복잡하지 않고, 훈련 데이터를 충분히 학습하지 못할 때 발생합니다. \n",
    "\n",
    "- Dropout : 은 학습 과정 중 신경망의 뉴런 중 일부를 랜덤하게 부분적으로 생략하는 방법론으로, 학습이 진행될 때마다 뉴런을 무작위로 학습하여, 매번 모델을 다르게 학습시킨다는 관점에서 앙상블 기법과 유사한 효과를 냅니다.\n",
    "\n",
    "- Parameter Norm Penalty : 손실함수에 Norm Penalty Term을 추가하여 사용하여 활용하며 L1 Norm, L2 Norm 등이 있다.\n",
    "\n",
    "- Batch Normalization : Batch Normalization(배치 정규화)는 배치단위간에 데이터 분포의 차이가 발생할 수 있기에, 학습 과정에서 각 배치 별로 데이터의 평균과 분산을 이용해 정규화를 진행하는 방법론으로, 각 데이터 분포를 평균은 0, 표준편차는 1인 데이터의 분포로 조정합니다.\n",
    "    - 배치 정규화는 초깃값의 영향을 크게 받지 않음.\n",
    "\n",
    "- Noise Robustness : 레이어 중간에 노이즈를 추가하여 과적합을 해결하는 방법론이다.\n",
    "\n",
    "- Early Stopping : 모델의 과적합을 방지하기 위해 이전 에폭(학습 반복 회수)에 비해 검증 오차(Validation Loss)가 감소하다 증가하는 경우 학습을 종료시킨다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975ca3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727e73f",
   "metadata": {},
   "source": [
    "## 6-6) 신경망 - 활성함수\n",
    "\n",
    "- 신경망에서 활성함수가 필요한 가장 적절한 이유 : 비선형 근사를 하기 위해서\n",
    "\n",
    "- 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수라고 합니다. \n",
    "\n",
    "    - ‘활성화'라는 이름에서 알 수 있듯이, 활성화 함수는 입력 신호의 총합이 활성화를 일으키는 지 결정하는 역할을 수행하고 있습니다. \n",
    "\n",
    "    - 이러한 활성화 함수는 임계값을 경계로 출력이 바뀌게 됩니다. \n",
    "\n",
    "    - 주로 사용되는 활성화 함수는 시그모이드 함수 등이 있습니다. 활성함수를 사용하지 않으면 신경망은 선형 모델과 차이가 없습니다.\n",
    "    \n",
    "- sigmoid 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a04a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55928b",
   "metadata": {},
   "source": [
    "## cross-entropy H(P,Q),  KL-divergence KL (X|Y)\n",
    "\n",
    "\n",
    "흔히 classification task에서는 cross-entropy를 목적식으로 활용합니다. \n",
    "\n",
    "후에 다른 강의들에서는 KL-divergence loss 등을 확인할 수 있을텐데요. KL-divergence는 확률분포 사이의 차이를 비교할 때 사용하는 방법 중의 하나입니다. \n",
    "\n",
    "가령, 확률 분포 P를 모델링하여 확률 분포 Q를 얻었을 때, 모델링한 Q가 원래의 P와 얼마나 차이가 나는 지 확인할 때 사용합니다. \n",
    "\n",
    "\n",
    "- 이 때, 주어진 확률분포 P, Q 에 대하여, cross-entropy H(P,Q) 수식을 entropy H(X) 와 KL-divergence KL (X|Y) 로 표현할 수 있는데요. 수식이 아래와 같을 때, cross-entropy와 KL-divergence의 관계를 올바르게 표현한 것은 어떤 것일까요?\n",
    "\n",
    "---\n",
    "\n",
    "- H(P, Q)는 P와 Q 간의 cross-entropy입니다.\n",
    "- H(P)는 P의 entropy (불확실성, 정보량)입니다.\n",
    "- KL(P || Q)는 P와 Q 간의 KL-divergence (Kullback-Leibler divergence)입니다.\n",
    "\n",
    "\n",
    "이 수식은 다음과 같이 해석할 수 있습니다:\n",
    "\n",
    "- cross-entropy H(P, Q)는 P의 entropy H(P)와 P와 Q 간의 KL-divergence KL(P || Q)의 합으로 표현됩니다. \n",
    "- 즉, H(P, Q)는 P와 Q 간의 차이를 측정하는 부분인 KL-divergence와 P의 불확실성을 나타내는 entropy를 합친 값입니다.\n",
    "\n",
    "따라서 올바른 표현은 다음과 같습니다:\n",
    "- cross-entropy H(P, Q) = H(P) + KL(P || Q)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73858734",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cd62d",
   "metadata": {},
   "source": [
    "## 6-8) 경사하강법\n",
    "\n",
    "다음 중 경사하강법에 대한 설명 중 옳지 않은 것을 고르세요.\n",
    "\n",
    "\n",
    "1. 기계학습 문제는 대부분 학습 단계에서 최적의 매개변수(가중치와 편향)를 찾아야 한다. \n",
    "\n",
    "2. 이때, 기울기를 통해 함수의 최솟값, 혹은 가능한 한 작은 값을 찾으려 하는 것을 경사법이라고 한다.\n",
    "\n",
    "3. 미분이 가능한 함수 f에 대해 주어진 점 (x, f(x))에서의 접선의 기울기가 음수라면, x를 증가시키면 함수값 f(x)가 감소한다.\n",
    "\n",
    "4. 경사하강법은 비볼록 함수, 안장점 등에서의 한계점이 존재한다.\n",
    "\n",
    "---\n",
    "\n",
    "5. (x) 경사하강법을 통해 구한 함수의 극소값의 위치는 해당 함수의 최솟값의 위치이다.\n",
    "\n",
    "    - 경사하강법을 통해 구한 함수의 극소값(또는 극대값)의 위치가 해당 함수의 최솟값(또는 최댓값)의 위치인지 여부는 함수의 형태와 경사하강법의 초기 조건에 따라 다릅니다.\n",
    "    - 5-1) 볼록 함수 (Convex Function)의 경우, 경사하강법을 사용하여 구한 함수의 극소값(극소점)의 위치는 해당 함수의 최솟값(전역 최솟값)의 위치와 일치합니다. \n",
    "    - 5-2) 비볼록 함수(Non-convex Function 는 여러 개의 극소점(local minima)을 가질 수 있으며, 경사하강법은 초기 조건에 따라 수렴 지점이 다를 수 있습니다. 초기 조건에 따라 수렴 지점이 다를 수 있으며, 최솟값이 아닌 극소값 지역 최솟값(local minimum)에 수렴할 수도 있습니다. \n",
    "    \n",
    "    - 5-3) 안장점 (Saddle Points): 안장점은 기울기가 0이지만 극소점이나 극대점이 아닌 지점을 가리킵니다. 경사하강법은 안장점에서 기울기가 0이므로 멈출 수 있으며, 전역 최솟값이 아닌 지점에서 수렴할 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- 경사 하강법 학습률, 학습 횟수 잘 조절해서 원하는 계수를 찾을 수 있음.\n",
    "- 확률적 경사 하강법 SGD 에서는 학습률, 학습 횟수, 미니 배치 사이즈 까지 고려\n",
    "- 일부 데이터만 사용해서 처리, 처리 속도 빠름. 배치사이즈가 너무 작으면 오히려 늦을 수도 있음\n",
    "- 최소점으로 가는 방향 은 향함\n",
    "- 경사 하강법 처럼, 모든 데이터를 처리하게되면, 메모리가 부족하여 Out of memory 발생\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad703e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b0c96",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2c4ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171672bd",
   "metadata": {},
   "source": [
    "## 시퀀스 데이터, RNN\n",
    "\n",
    "시퀀스 데이터에대한 설명 중 가장 적절한 것을 고르세요.\n",
    "\n",
    "1. (x) 대표적인 시퀀스 데이터에는 소리 데이터, 문자열 데이터, 주가 데이터, 이미지 데이터 등이 있다.\n",
    "    - 시퀀스 데이터는 과거 정보 또는 맥락과 관련이 있는 데이터를 뜻합니다. 이미지 데이터는 맥락 혹은 과거 정보와 크게 상관이 없는 경우가 많습니다.\n",
    "\n",
    "---\n",
    "\n",
    "2. (x) 시퀀스 데이터를 학습시켜 모델을 생성하는 시퀀스 모델링은 일대다, 혹은 다대일 모델링만이 가능하다.\n",
    "\n",
    "    - 시퀀스 데이터를 학습시켜 모델을 만드는 시퀀스모델링은 크게 다대일, 일대다, 다대다 모델링 세 가지가 가능합니다. \n",
    "    -  다대일 모델링은 입력 데이터는 시퀀스, 출력 데이터는 고정 크기의 벡터인 경우로, 텍스트 데이터를 입력하여 감성 분류를 시행하는 감성 분석 등이 이에해당합니다. \n",
    "    - 일대다 모델링은 일반적인 형태의 데이터를 입력하여 시퀀스 데이터 출력값을 받는 모델링으로, 이미지 데이터에서 캡션 텍스트 출력을 받는 이미지 캡셔닝 등이 이에 해당합니다. \n",
    "    - 다대다 모델링은 입출력 배열이 모두 시퀀스 데이터인 경우로, 한 가지 언어의 텍스트 데이터를 입력받아 다른 언어 텍스트 데이터로 출력하는 번역 등이 이에 해당합니다.\n",
    "    \n",
    "---\n",
    "\n",
    "3. RNN의 잠재변수는 다음 순서의 잠재 변수를 모델링하기 위한 입력 값으로 사용될 수 있다.\n",
    "    - RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링합니다. \n",
    "    - 따라서, 다음 순서의 잠재 변수 모델링을 위한 입력값으로 활용할 수 있습니다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "4. (x) LSTM의 입력(Input) 게이트는 새로운 정보 중 어떤 것을 버릴 지, 망각(Forget) 게이트는 새로운 정보 중 어떤 것을 저장할 지 결정하게 된다.\n",
    "    -  LSTM의 입력(Input) 게이트는 새로운 정보 중 어떤 것을 저장할 지 결정하며, LSTM의 망각(Forget) 게이트는 새로운 정보 중 어떤 부분을 버릴 지 말지 결정합니다.\n",
    "\n",
    "---\n",
    "\n",
    "5. (x) 시퀀스의 길이가 길어지는 경우, Vanila RNN에서 발생할 수 있는 기울기 소실 문제를 해결하기 위해 LSTM, GRU, CNN 등의 기법을 사용할 수 있다.\n",
    "    - CNN은 기울기 소실 문제를 해결하기 위해 고안된 방안이 아닙니다. \n",
    "    - RNN의 기울기 소실 문제 해결을 위해 고안된 네트워크에는 LSTM, GRU 등이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc70b7a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463c0d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63b0c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f57193",
   "metadata": {},
   "source": [
    "# 5기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e7f436",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850622f",
   "metadata": {},
   "source": [
    "## 5-1) Overfitting, Underfitting\n",
    "\n",
    "\n",
    "- Overfitting은 훈련 데이터의 개수에 비해 사용하는 모델이 너무 큰 경우 발생할 수 있다.\n",
    "- Underfitting이 일어났을 경우 학습을 더 오래 진행하거나 더 큰 모델을 활용하여 해결 가능하다.\n",
    "- Overfitting이 일어났을 경우 Early Stop 을 적용하면 효과를 볼 수 있다.\n",
    "- Overfitting이 일어난다면 Dropout 기법을 적용하면 효과를 볼 수 있다.\n",
    "\n",
    "-- Overfitting이 일어났을 경우 모델에 regularization method를 적용해본다. (Underfitting x)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Overfitting (과적합)은 모델이 학습 데이터셋을 지나치게 학습하여 발생하는 문제로 학습 데이터셋에서는 성능이 높게 나타나지만, 평가 데이터셋(일반화된 데이터셋)에서 오히려 성능이 더 떨어지는 현상을 의미한다. \n",
    "\n",
    "Overfitting이 발생한다면 훈련데이터를 추가하거나 더 작은 모델을 활용해볼 수 있다. \n",
    "\n",
    "또한, Dropout, Regularization Method, Early Stopping Rule 등등의 방법론을 활용하기도 한다.\n",
    "\n",
    "\n",
    "Underfitting(과소적합)은 Overfitting(과적합)의 반대 개념으로 모델이 충분히 학습 데이터를 학습하지 않은 상태를 의미한다. Underfitting이 발생한다면 학습을 더 오래 진행하거나 더 큰 모델을 활용하여 문제를 해결 할 수 있다.\n",
    "\n",
    "Underfitting이 아닌 Overfitting이 일어났을 경우, 모델에 regularization method를 적용하는 것이 옳기에 정답은 4번이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9fa92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc212e53",
   "metadata": {},
   "source": [
    "## 5-2) Gradient Descent 기법들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e30072",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6442b48",
   "metadata": {},
   "source": [
    "## 5-3) 정규화 방식\n",
    "\n",
    "딥러닝에서는 Overfitting을 방지하기 위해서 다양한 정규화 방법론을 활용합니다.\n",
    "\n",
    "- Dropout은 학습 과정 중 신경망의 뉴런 중 일부를 랜덤하게 부분적으로 생략하는 방법론으로 학습이 진행될 때마다 뉴런을 무작위로 학습하여 매번 모델을 다르게 학습시킨다는 관점에서 앙상블 기법과 유사한 효과를 냅니다.\n",
    "\n",
    "- Parameter Norm Penalty는 L1 Norm, L2 Norm등이 있으며 손실함수에 해당 Norm Penalty Term을 추가적으로 활용하여 정규화를 진행합니다. L1 Norm은 오차의 절댓값을 L2 Norm은 오차 제곱합을 활용합니다.\n",
    "\n",
    "- (x) Batch Normalization(배치 정규화)는 배치단위간에 데이터 분포의 차이가 발생할 수 있기에, 학습 과정에서 각 배치 별로 데이터의 평균과 분산을 이용해 정규화를 진행하는 방법론으로 각 데이터 분포를 평균은 0, 표준편차는 1인 데이터의 분포로 조정합니다.\n",
    "\n",
    "- Noise Robustness는 레이어 중간에 노이즈를 추가하여 학습 효과를 높이고 과적합을 방지하는 방법론이다.\n",
    "\n",
    "- 마지막으로 Early Stopping은 모델의 과적합을 방지하기 위해서 이전 에폭에 비해 검증 오차(Validation Loss)가 감소하다 증가하는 경우 학습을 종료시키는 방법론입니다. 그러므로 정답은 5번입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728410ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab43f021",
   "metadata": {},
   "source": [
    "## 5-4) RNN 계열 모델 Vanilla RNN, LSTM, GRU\n",
    "\n",
    "\n",
    "- Vanilla RNN은 이전 Cell의 output 값이 현 시점의 input과 함께 연산이 되는 방법론입니다. \n",
    "\n",
    "- 하지만 Vanilla RNN에서는 시퀀스가 매우 길어지면 앞쪽 정보가 뒤에 있는 타임 스텝까지 충분히 전달되지 못하는 장기의존성(Long Term Dependency) 문제를 해결하지 못합니다.\n",
    "\n",
    "---\n",
    "\n",
    "- LSTM(Long Short Term Memory)은 입력(input) 게이트, 망각(forget) 게이트, 출력(Output) 게이트를 활용하여 장기의존성 문제를 해결하고자 합니다. \n",
    "\n",
    "- 입력게이트는 새로운 정보 중 어떤 것을 저장할지 결정하는 게이트입니다. \n",
    "\n",
    "- 또한, 망각게이트는 과거의 정보 중 어떤 부분을 버릴지 말지 결정하는 게이트이며, \n",
    "\n",
    "- 출력게이트는 입력된 데이터 중 정보 어떤 정보를 출력으로 내보낼 지 결정하는 게이트입니다.\n",
    "\n",
    "---\n",
    "\n",
    "GRU 는 LSTM의 변형구조로 Update Gate, Reset Gate만 존재하며 Output Gate는 존재하지 않아 LSTM에 비해 파라미터 수가 더 작은 모델이다.\n",
    "(update reset) output x\n",
    "\n",
    "\n",
    "이에, 선지의 잘못된 내용을 정정하면 다음과 같다.\n",
    "\n",
    "\n",
    "\n",
    "- Vanilla RNN에서는 시퀀스가 매우 길어도 앞쪽 정보가 뒤에 있는 타임 스텝까지 충분히 전달 되지 못한다.\n",
    "\n",
    "\n",
    "- LSTM은 Vanilia RNN보다 장기의존성(Long Term Dependency)을 해결한다.\n",
    "\n",
    "- LSTM의 망각(Forget) 게이트는 과거의 정보를 버릴지 말지 결정한다.\n",
    "\n",
    "- GRU는 Reset, Update 게이트로 이루어져 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c50b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64f93d6",
   "metadata": {},
   "source": [
    "## 5-5) stride, pad, 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0242eea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6946b4c",
   "metadata": {},
   "source": [
    "## 5-6) 피처맵 크기\n",
    "32 - \n",
    "s + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1e027",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e6022",
   "metadata": {},
   "source": [
    "## 5-7) 시그모이드 함수, 순전파 역전파 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb6a7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7c7d5",
   "metadata": {},
   "source": [
    "## 5-8) 선형독립, 선형 종속\n",
    "\n",
    "\n",
    "1. 선형 독립이면 선형방정식의 해가 유일하다. \n",
    "\n",
    "2. 벡터 v1, v2 가주어졌을때, 새로운 벡터 v3가 Span(vl,v2}에 포함된다면 이는 선형 종속 을의\n",
    "미한다. \n",
    "\n",
    "3. vi 가 1부터 n까지의 벡터라고 할때, x1v1 + x2v2 + ⋯ + xnvn = 0 을만족하는x1, x2, ..., xn 이 모두 0이면 선형 독립이다.\n",
    "\n",
    "4. (x) x1v1 + x2v2 + ⋯ + xnvn = 0을 만족하는 해인 x1, x2, ..., xn 이 모두 0이 아닐 경우에만 선형 종속이다. \n",
    "    - 모두 x, 0이 아닌 xi가 하나라도 존재하면 선형 종속\n",
    "\n",
    "5. 벡터의 집합중 어떤 벡터의 실수배 인 벡터가 존재하는 경우 이는 선형 종속 이다.\n",
    "\n",
    "---\n",
    "\n",
    "- 선형 독립(LinearlyIndependent)은 v1, v2, ..., vn 의 벡터중 하나인 vj가 이전 벡터들의 선형결합으로 표현 되지 않는 경우를 의미합니다.\n",
    "\n",
    "    - 즉, x1v1 + x2v2 + ⋯ + xnvn = 0을 만족하는 해인 x1, x2, ..., xn 이 모두 0인 경우를 의미합니다.\n",
    "    \n",
    "    - 다시 말하면, 오직 영벡터만이 주어진 벡터들의 선형 결합으로 나타낼 수 있는 경우입니다. \n",
    "\n",
    "    - 그러므로 선형 독립인 경우 선형방정식의 해는 유일 합니다. \n",
    "    \n",
    "    - 선형 독립인 벡터들은 서로 \"독립적\"이며, 하나의 벡터를 다른 벡터들의 선형 조합으로 나타낼 수 없습니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 선형종속(LinearlyDependent)은 선형 독립과 반대로 v1, v2, ..., vn 의 벡터중 하나인 vj가 이전 벡터들의 선형결합 으로 표현되는 경우를 의미합니다.\n",
    "\n",
    "    - 즉, x1v1 + x2v2 + ⋯ + xnvn = 0을 만족하는 해인 x1, x2, ..., xn, 중 0이 아닌 xi가 존재하는 경우를 의미합니다.\n",
    "\n",
    "    - 그러므로 선형 종속인 경우 선형방정식의 해는 여러개 가 존재한다. \n",
    "\n",
    "---\n",
    "\n",
    "여기서 선형종속의 기하학적 의미를 살펴봅시다.\n",
    "\n",
    "- 2개의 벡터인 v1,v2가 존재한다고 가정했을때, 이는 기하학적 으로 평면인 Span(v1,v2)를 나타냅니다.\n",
    "\n",
    "- 여기서 Span 이란 벡터들의 선형결합 으로 나타낼 수 있는 모든 벡터의 집합을 의미합니다.\n",
    "\n",
    "- 만약 v3 가 v3= 2v1 + 3v2 등 v1,v2의 선형 결합으로 표현된다면 이는 v3가 해당 평면 안에 표현되며 v3 E Span(v1,v2)를 의미한다는 것을 알수 있습니다.\n",
    "\n",
    "- 결과적으로 하나의 벡터가 이전벡터들의 선형결합으로 표현 가능한 선형종속은 Span을 증가시키지 못하고Span(v1, v2)와 동일 해집니다.\n",
    "\n",
    "- 선형종속은 x1v1 + x2v2 + ⋯ + xnvn = 0을 만족하는 x1, x2, ..., xn 중 0이 아닌 xi가 존재하는경우 이기 때문에 정답은 4번입니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea126e7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd214b4",
   "metadata": {},
   "source": [
    "##  5-9) 선형변환 에 대한 표준 행렬\n",
    "\n",
    "1-1). [x1, x2, x3]는 R^3 공간의 벡터이고, [x1 + 2x2, 3x2 - 2x3]는 R^2 공간의 벡터입니다. \n",
    " \n",
    "1-2). 이 선형변환은 R^3에서 R^2로의 매핑을 나타냅니다.\n",
    "\n",
    "2-1). 이 선형변환을 표준행렬로 나타내기 위해, R^3의 표준기저인 e1, e2, e3를 이용하여 T(e1), T(e2), T(e3)을 계산\n",
    "\n",
    "2-2). T(e1) 계산:\n",
    "\n",
    "```\n",
    "T([1, 0, 0]) = [1 + 2 * 0, 3 * 0 - 2 * 0] = [1, 0]\n",
    "\n",
    "T(e2) 계산:\n",
    "T([0, 1, 0]) = [0 + 2 * 1, 3 * 1 - 2 * 0] = [2, 3]\n",
    "\n",
    "T(e3) 계산:\n",
    "T([0, 0, 1]) = [0 + 2 * 0, 3 * 0 - 2 * 1] = [0, -2]\n",
    "````\n",
    "\n",
    "표준행렬 A는 선형변환 T(e1), T(e2), T(e3)을 열 벡터로 가지는 행렬입니다`\n",
    "\n",
    "```\n",
    "A = | 1  2  0 |\n",
    "    | 0  3 -2 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b860437",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd6a0d",
   "metadata": {},
   "source": [
    "## 5-10) 부분공간(Subspace), 기저(Basis), 열 공간(Column Space), 계수(Rank) \n",
    "\n",
    "- 부분공간은 선형 결합에 닫혀있는 벡터공간의 부분집합이다.\n",
    "\n",
    "- 기저는 모든 재료벡터들이 선형 독립이고 해당 벡터들이 부분공간 전체를 표현 가능한 벡터들의 집합을 의미한다.\n",
    "\n",
    "- 열 공간은 행렬의 열 벡터(Column Vector)를 선형결합하여 얻어지는 벡터 공간을 의미한다.\n",
    "\n",
    "- 계수(Rank)는 열 공간의 차원 수를 의미합니다.\n",
    "\n",
    "\n",
    "x 기저는 항상 유일하며 기저를 구성하는 벡터의 개수인 차원(Dimension)도 유일하다.\n",
    "\n",
    "    - 기저는 벡터 공간에 따라 다를 수 있으며, 기저를 구성하는 벡터의 개수도 다를 수 있습니다. \n",
    "\n",
    "    - 한 벡터 공간은 여러 가지 서로 다른 기저를 가질 수 있습니다. \n",
    "\n",
    "     - 저의 개수는 해당 벡터 공간의 차원을 나타내며, 차원은 유일하지만 기저 자체는 유일하지 않을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d694af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9068e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5e4af",
   "metadata": {},
   "source": [
    "# 1,2,3,4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fbecd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff59ba2",
   "metadata": {},
   "source": [
    "## 4-4) RNN\n",
    "\n",
    "\n",
    "- RNN은 hidden 노드가 방향을 가진 엣지로 연결되어 순환구조를 이루는 인공신경망의 한 종류이다.\n",
    "\n",
    "- (x) RNN은 input 길이에 비례하여 모델 parameter가 증가한다.\n",
    "    - RNN의 모델 파라미터 수는 입력 시퀀스의 길이와는 무관하며, 고정된 수의 파라미터를 가집니다. RNN의 파라미터 수는 hidden state의 크기, 가중치 행렬 등과 관련이 있지만 입력 시퀀스의 길이와 직접적으로 비례하지 않습니다.\n",
    "\n",
    "- RNN은 input 길이가 길어질수록, 역전파시 그래디언트가 점차 줄어 학습능력이 떨어진다.\n",
    "    - 시퀀스가 길어질수록, 기울기 소실 문제 > GRU, LSTM 등이 나옴\n",
    "\n",
    "- RNN의 변형 종류로는 GRU, LSTM 등이 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a90a9b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99289bb6",
   "metadata": {},
   "source": [
    "## 3-4) \n",
    "- 학습 데이터셋의 총 샘플 수가 1,025개이고, batch size를 32로 하여 모델을 학습시킬 경우, 모델이 모든 데이터를 2번씩 보고 배우는 데에 필요한 step수는?\n",
    "\n",
    "    - 1025개의 샘플을 2번 본다는 것은 2050개의 데이터를 봐야한다는 말과 같습니다. \n",
    "\n",
    "    - 2050개의 데이터를 32번씩 볼 경우 2050 / 32 = 64.0625, \n",
    "    \n",
    "    - 32x64 = 2048로, 데이터 2개를 더 봐야함. \n",
    "    \n",
    "    - 따라서 64회 반복하고도 샘플 2개가 아직 남았기 때문에 1 step이 더 필요합니다. 따라서 65회가 정답입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d633bde8",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bdd9b3",
   "metadata": {},
   "source": [
    "## 3-5\n",
    "\n",
    "- Quick Sort - O(nlogn)\n",
    "- Merge Sort - O(nlogn)\n",
    "- Insertion Sort - O(n^2)\n",
    "- Bublle Sort - O(n^2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee978e2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88c8c9",
   "metadata": {},
   "source": [
    "## 1-5)  learning step의 수\n",
    "\n",
    "- learning step의 수 = (총 샘플 수) / (batch size)\n",
    "    - learning step의 수 = 10,000 / 100 = 100\n",
    "    - 따라서 모든 데이터를 학습에 사용하려면 100개의 learning step이 필요합니다. \n",
    "\n",
    "- 각 learning step마다 모델은 batch size에 해당하는 100개의 샘플을 사용하여 가중치를 업데이트합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fc6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(num):\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if num % 5 == 0:\n",
    "            cnt = cnt + (num // 5)\n",
    "            break\n",
    "        \n",
    "        \n",
    "        num = num - 3\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "        if num <= 0:\n",
    "            break\n",
    "            \n",
    "    return cnt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

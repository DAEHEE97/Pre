{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41648561",
   "metadata": {},
   "source": [
    "# 신경망(Neural Network)\n",
    "\n",
    "- 선형모델은 단순한 데이터를 해석할 때는 유용하지만 분류문제나 좀 더 복잡한 패턴의 문제를 풀 때는 예측성공률이 높지 않습니다. \n",
    "\n",
    "- 이를 개선 -> 비선형모델인 신경망(neural network)\n",
    "\n",
    "- 신경망의 구조와 내부에서 사용되는 softmax, 활성함수, 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486b116",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ee07c",
   "metadata": {},
   "source": [
    "- 각 행벡터 oi 는 데이터 xi 와 가중치 행렬 W 사이의 행렬곱과 절편 b 벡터의 합으로 표현\n",
    "\n",
    "- 데이터가 바뀌면 결과값도 바뀌게 됩니다. 이 때 출력 벡터의 차원은 d 에서 p 로 바뀌게 됩니다.\n",
    "\n",
    "- d 개의 변수로 p 개의 선형모델을 만들어서 p 개의 잠재변수를 설명하는 모델을 상상해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8380b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac62664",
   "metadata": {},
   "source": [
    "# 소프트맥스 연산\n",
    "\n",
    "- 소프트맥스(softmax) 함수는 *모델의 출력을 확률로 해석*할 수 있게 변환해 주는 연산입니다\n",
    "    - 학습(training) 단계에서 사용되어 출력을 확률 분포로 정규화\n",
    "\n",
    "- 분류 문제를 풀 때, 선형모델과 소프트맥스 함수를 결합하여 예측합니다.\n",
    "\n",
    "- 출력 벡터 o 에 softmax 함수를 합성하면 확률 벡터가 되므로, 특정 클래스 k 에 속할 확률로 해석할 수 있다.\n",
    "\n",
    "- 그러나 추론을 할 때는 원-핫(one-hot) 벡터로 최대값을 가진 주소만, 1로 출력하는 연산을 사용해서 softmax 를 사용하진 않습니다\n",
    "\n",
    "---\n",
    "\n",
    "# 원-핫(one-hot) 벡터\n",
    "\n",
    "- 소프트맥스 연산은 주로 학습(training) 단계에서 사용됩니다. \n",
    "\n",
    "- 학습 과정에서 소프트맥스 함수를 사용하여 모델의 출력을 확률 분포로 정규화하고, 손실 함수를 계산하여 모델을 학습합니다.\n",
    "\n",
    "- 추론(inference) 단계에서는 일반적으로 소프트맥스 함수를 사용하지 않습니다. \n",
    "\n",
    "- 대신, 모델의 출력에서 가장 큰 값을 가진 클래스를 선택하기 위해 원-핫(one-hot) 벡터로 최대값을 가진 주소만을 1로 출력하는 연산을 사용합니다. 이것은 소프트맥스 함수의 역할을 대신합니다.\n",
    "\n",
    "- 따라서 소프트맥스 함수는 학습 단계에서 주로 사용되며, 추론 단계에서는 일반적으로 원-핫 인코딩과 같은 다른 방법을 사용하여 예측을 수행합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98a52d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a715a9d4",
   "metadata": {},
   "source": [
    "#  활성함수(activation function)\n",
    "\n",
    "- 활성함수(activation function)는 R 위에 정의된 비선형(nonlinear) 함수 로서,딥러닝에서 매우 중요한 개념입니다.\n",
    "    - 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없습니다.\n",
    "    \n",
    "- 시그모이도(sigmoid) 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성 함수지만, 딥러닝에선 ReLU 함수를 많이 쓰고 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "# 2층(2-layers) 신경망\n",
    "\n",
    "- 신경망(Neural Network)은 선형모델과 활성함수(activation function)를 합성한 함수입니다.\n",
    "\n",
    "- 활성함수 σ 는 비선형함수로 잠재벡터 z = (z1, ..., zq) 의 각 노드에 개별적으로 적용하여, 새로운 잠재벡터 H = (σ(z1), ..., σ(zn)) 를 만든다.\n",
    "\n",
    "- 잠재벡터 H 에서 가중치 행렬 W(2) 와 b(2) 를 통해 다시 한 번 선형변환해서 출력하게 되면, (W(2), W(1)) 를 패러미터로 가진 2층(2-layers) 신경망이다\n",
    "\n",
    "- 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수입니다.\n",
    "    - MLP 의 패러미터는 L 개의 가중치 행렬 W(L), ..., W(1) 과 로 이루어져 있다.\n",
    "    \n",
    "    - l = 1,..., L 까지 순차적인 신경망 계산을 순전파(forward propagation)라 부른다\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286bf03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8562a",
   "metadata": {},
   "source": [
    "- 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있습니다\n",
    "    - 이를 universal approximation theorem 이라 부릅니다.\n",
    "    \n",
    "- 층이 얇으면 (2충) 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은(wide) 신경망이 되어야 한다.\n",
    "    \n",
    "- 따라서 층이 깊을수록, 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어, 좀 더 효율적으로 학습이 가능합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e917cbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cb060",
   "metadata": {},
   "source": [
    "# 역전파(backpropagation)\n",
    "\n",
    "- 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분 (auto-differentiation)을 사용합니다\n",
    "    - 각 노드의 텐서 값을 컴퓨터가 기억해야 미분 계산이 가능하다.\n",
    "    - 저장의 기능이 필요, 메모리 하드 좀더 필요 \n",
    "- 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 패러미터 {W(l), b(l)}Ll=1를 학습합니다.\n",
    "\n",
    "- 손실함수를 L 이라 했을 때 역전파는 ∂L/∂W(l) 정보를 계산할 때 사용된다\n",
    "\n",
    "- 각 층 패러미터의 그레디언트 벡터는 윗 층 부터 역순으로 계산하게 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61b971",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74eab69",
   "metadata": {},
   "source": [
    "# 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4daac1",
   "metadata": {},
   "source": [
    "- 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하 기 위한 도구로 최적화에서 제일 많이 사용하는 기법입니다.\n",
    "\n",
    "- 미분은 변화율의 극한(limit) 으로 정의합니다.\n",
    "\n",
    "- sympy.diff 를 가지고 미분 계산 가능\n",
    "\n",
    "- 미분을 계산하려면 함수의 모양이 매끄러워야(연속) 한다.\n",
    "\n",
    "---\n",
    "\n",
    "- 미분은 함수 f 의 주어진 점 (x, f(x)) 에서의 접선의 기울기를 구한다.\n",
    "\n",
    "- 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있다\n",
    "\n",
    "---\n",
    "\n",
    "- 미분값을 더하면 경사상승법(gradient ascent)이라 하며 함수의 극대값\n",
    "의 위치를 구할 때 사용한다.\n",
    "    \n",
    "    - 목적함수를 최대화할 때 사용한다.\n",
    "\n",
    "- 미분값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값\n",
    "의 위치를 구할 때 사용한다.\n",
    "\n",
    "    - 목적함수를 최소화할 때 사용한다.\n",
    "    \n",
    "- 경사상승/경사하강 방법은 극값에 도달하면 움직임을 멈춘다.\n",
    "   \n",
    "   - 극값에선 미분값이 0 이므로 더 이상 업데이트가 안 된다.\n",
    "   - 그러므로 목적함수 최적화가 자동으로 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c98de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6d10a",
   "metadata": {},
   "source": [
    "- gradient: 미분을 계산하는 함수,  init: 시작점\n",
    "\n",
    "- 컴퓨터로 계산할 때 미분이 정확히 0이 되는 것은 불가능하므로, eps(알고리즘 종료조건) 보다 작을 때 종료하는 조건이 필요하다.\n",
    "\n",
    "- 이 부분이 `x − λf′(x)` 을 계산하는 부분. lr(학습률)은 학습률로서 미분을 통해 업데이트하는 속도를 조절한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80639f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6256a",
   "metadata": {},
   "source": [
    "- 미분(differentiation)은 **변수**의 움직임에 따른 함수값의 변화를 측정 하기 위한 도구로 최적화에서 제일 많이 사용하는 기법입니다.\n",
    "\n",
    "- 변수가 벡터라면, 벡터가 입력인 **다변수 함수의 경우** 편미분(partial differentiation)을 사용한다.\n",
    "\n",
    "\n",
    "\n",
    "- 각 변수 별로 편미분을 계산한 그레디언트(gradient) 벡터를 이용하여, 경사하강/경사상승법에사용할수있다\n",
    "\n",
    "- 앞서 사용한 미분값인 $f′(x)$ 대신 벡터 $∇f$ (nabla)를 사용하여, 변수 x = (x1, ..., xd) 를 동시에 업데이트 가능합니다.\n",
    "\n",
    "---\n",
    "\n",
    "- $f (x, y) = x^2 + 2y^2$\n",
    "\n",
    "- $∇f =(2x,4y)$\n",
    "    - ∇f(x,y)는 각점(x,y)에서 가장 빨리 증가하는 방향으로 흐르게 된다.\n",
    "    \n",
    "- $-∇f =-(2x,4y)$\n",
    "    - ∇f는 ∇(−f)랑 같고 이는 각 점에서 가장 빨리 감소하게 되는 방향과 같다.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11984e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70131b9",
   "metadata": {},
   "source": [
    "- `while(abs(grad) > eps)` -> `while(norm(grad) > eps)`\n",
    "    - 경사하강법 알고리즘은 벡터는 절대값 대신 노름(norm)을 계산해서 종료조건을 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd522d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f751ae1",
   "metadata": {},
   "source": [
    "# 경사하강법(Gradient Descent) 기반 선형회귀 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019e51d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403d7a5",
   "metadata": {},
   "source": [
    "\n",
    "- 선형모델의 경우 위와 같이 역행렬을 이용해서 회귀분석이 가능하다.역행렬을 이용하지 말고 경사하강법을 이용해 적절한 선형모델을 찾아보자.\n",
    "\n",
    "- 선형회귀의 목적식은 $∥y − Xβ∥2$ 이고, 이를 최소화하는 $β$ 를 찾아야 하므 로 다음과 같은 그레디언트 벡터를 구해야 한다.\n",
    "\n",
    "- ∥y − Xβ∥2 가 아닌 ∥y − Xβ∥2 를 최소화해도 된다\n",
    "\n",
    "- 목적식을 최소화하는 β 를 구하는 경사하강법 알고리즘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ce4ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725eaebe",
   "metadata": {},
   "source": [
    "- 이제 경사하강법 알고리즘으로 역행렬을 이용하지 않고 회귀계수를 계산할수 있다.\n",
    "\n",
    "\n",
    "- 경사하강법 알고리즘에선 lr(학습률)과 T(학습횟수)가 중요한 hyperparameter 가 된다.\n",
    "    - 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 **적절한 학습률과 학습횟수를 선택했을 때, 수렴이 보장**되어 있습니다.\n",
    "\n",
    "- 볼록한 함수는 그레디언트 벡터가 항상 최소점을 향한다.\n",
    "    - 볼록한 함수는 극소점(local minimum)이 없다.\n",
    "    - 볼록한 함수는 모든 극소점이 동시에 전역 최소점(global minimum)입니다. \n",
    "    - 이것은 볼록 함수의 중요한 특징 중 하나이며, 다른 어떤 지역 최소점(local minimum)도 없습니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 특히 *선형회귀의 문제의 경우* 목적식 $∥y − Xβ∥2$ 은 회귀계수 $β$ 에 대해 볼록함수 이기 때문에, 적절한 lr(학습률)과 T(학습횟수)를 선택했을 때 수렴이 보장됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd59813",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cc3e93",
   "metadata": {},
   "source": [
    "# 확률적 경사하강법(Stochastic Gradient Descent)\n",
    "\n",
    "- 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있습니다\n",
    "\n",
    "- 하지만 *비선형회귀 문제의 경우* 목적식이 볼록하지 않을 수 있으므로, 수렴 이 항상 보장되지는 않습니다.\n",
    "    \n",
    "- 특히 딥러닝을 사용하는 경우 목적식은 대부분 볼록함수가 아니다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1a30a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef69f93",
   "metadata": {},
   "source": [
    "- 확률적 경사하강법(stochastic gradient descent)은 모든 데이터를 사용해서 업데이트하는 대신 *데이터 한개 또는 일부만을 활용* 하여 업데이트합니다.\n",
    "\n",
    "- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있습니다.\n",
    "    - SGD 라고 해서 만능은 아니지만 딥러닝의 경우, SGD 가 경사하강법보다 실증적으로 더 낫다고 검증되었다\n",
    "    \n",
    "- SGD 는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에, 연산자원 을 좀 더 효율적으로 활용하는데 도움이 됩니다\n",
    "    - 전체 데이터 (X, y) 를 쓰지 않고 미니배치 (X(b), y(b)) 를 써서 업데이트 하므로 연산량이 b/n 로 감소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44d929",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3d2d8",
   "metadata": {},
   "source": [
    "- 경사하강법은 전체데이터 𝒟 = (X, y) 를 가지고 목적식의 그레디언트 벡터 인 ∇θL(𝒟,θ)를 계산합니다.\n",
    "\n",
    "- SGD 는 미니배치 𝒟(b) = (X(b), y(b)) ⊂ 𝒟 를 가지고 그레디언트 벡터를 계산합니다.\n",
    "    - 미니배치 𝒟(b) 를 가지고 목적식 의 그레디언트를 근사해서 계산.\n",
    "    - 매번 다른 미니배치를 사용하기 때문에 곡선 모양이 바뀌게 된다.\n",
    "    - SGD 는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 효율적입니다.\n",
    "    - 만일 일반적인 경사하강법처럼 모든 데이터를 업로드하면 메모리가 부족하여 Out-of-memory 가 발생한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69341b2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
